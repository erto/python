import sysdef parse_sentences(filename):  """Parse parser output into a dictionary of dictionaries"""    # Input is the output from Libin Shen's LTAG-spinal dependency parser  f = open(filename)  in_sentences = f.read()  f.close()  # Initialize sentence counter and sentence record  next_sen_num = 0  out_sentences = []  # For each sentence (separated by "\n\n")  for in_sentence in in_sentences.split("\n\n"):    if in_sentence == "":      break    in_words = []    # Words within a sentence are separated by "\n#"    sentence_iter = iter(in_sentence.split("\n#"))    # A sentence's first two lines contain sentence number and root index    (sen_num, root_num) = sentence_iter.next().split("\n")    # Format e.g. "^root 4$"    root_num = root_num.split("root ")[1]    (sen_num, root_num) = (int(sen_num), int(root_num))    # Ensure that this is the expected sentence number    if sen_num != next_sen_num:      raise hell    else:      pass #if debug_mode:      #  print "Processing sentence", sen_num    for in_word in sentence_iter:      word_iter = iter(in_word.split("\n"))      # First line has word num and word. Format e.g. "^0 The$"      (word_num, word_text) = word_iter.next().split(" ")      (word_num, word_text) = (int(word_num), word_text.lower())      # Next line has POS. Format e.g. "^ pos: MD$"      word_pos = word_iter.next().split(" pos: ")[1]      # Next 0 or more lines have dependencies. Format e.g. "^ att #2$"      word_deps_pre = []      word_deps_post = []      conj_words = []      for dep in word_iter:        d1 = dep.split(" att #")        if len(d1) == 2:          dep_num = int(d1[1])          if dep_num < word_num:            word_deps_pre.append(dep_num)          else:            word_deps_post.append(dep_num)        else:          d2 = dep.split(" adj #")          if len(d2) == 2:            dep_num = int(d2[1])            if dep_num < word_num:              word_deps_pre.append(dep_num)            else:              word_deps_post.append(dep_num)          else:            if dep[0] == "&" and word_iter.next() == " pos: NA":              conj_deps = []              for conj in word_iter:                conjeme = conj.split(" con #")                if len(conjeme) == 2:                  conj_deps.append(int(conjeme[1]))                elif conj[0] == "&" and word_iter.next() == " pos: NA":                  conj_deps = tuple(conj_deps)                  conj_words.append(("&", "&", conj_deps, ()))                  conj_deps = []                  conj = word_iter.next()                else:                  raise hell              conj_deps = tuple(conj_deps)              conj_words.append(("&", "&", conj_deps, ()))            else:              print "Unrecognized dep:", dep              raise hell      # Combine word data      word_deps_pre = tuple(word_deps_pre)      word_deps_post = tuple(word_deps_post)      in_word = (word_text, word_pos, word_deps_pre, word_deps_post)            # Add word to sentence. If we found conj "words", add those too      in_words.append(in_word)      in_words.extend(conj_words)    in_words = tuple(in_words)    out_sentences.append((root_num, in_words,))        next_sen_num += 1  out_sentences = tuple(out_sentences)  return out_sentencesdef print_preceding_pos(sentences):  """Print the POSes of top-level words"""  for (root_index, words) in sentences:    root_word = words[root_index]    poses = ""    wordses = ""    before_root = True    for dep in root_word[2]:      if before_root and (dep > root_index):        before_root = False        poses += "__ "        wordses += words[root_index][0] + " "      poses += words[dep][1] + " "      wordses += words[dep][0] + " "    sentence = " ".join([word[0] for word in words])    print poses    print wordses    print sentence  return  def calculate_attachment_matrix(sentences):  """Compute how often each word takes each other word as an attachment."""  words = {}  for (_, sen_words) in sentences:    for word in sen_words:      word_text = word[0] + "_" + word[1]      if word_text not in words:        words[word_text] = ({}, {})      for dep_index_pre in word[2]:        dep_word = sen_words[dep_index_pre]        dep_text = dep_word[0] + "_" + dep_word[1]        if dep_text not in (words[word_text])[0]:          words[word_text][0][dep_text] = 1        else:          words[word_text][0][dep_text] += 1      for dep_index_post in word[3]:        dep_word = sen_words[dep_index_post]        dep_text = dep_word[0] + "_" + dep_word[1]        if dep_text not in (words[word_text])[1]:          words[word_text][1][dep_text] = 1        else:          words[word_text][1][dep_text] += 1  return wordsdef main(filename):  parsed = parse_sentences(filename)  collated = calculate_attachment_matrix(parsed)  for (index, word) in enumerate(collated):    if index > 100:      break    else:      print word + collated[word]  returnif __name__ == "__main__":  main(sys.argv[1])